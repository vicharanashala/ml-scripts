# Agricultural Data Processing Pipeline

A comprehensive pipeline for processing, cleaning, deduplicating, and generating English Q&A pairs from agricultural query datasets.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Punjab Paddy Pipeline](#punjab-paddy-pipeline)
- [Quick Start](#quick-start)
- [Data Processing Workflows](#data-processing-workflows)
  - [1. Data Cleaning & Concatenation](#1-data-cleaning--concatenation)
  - [2. Question Deduplication](#2-question-deduplication)
  - [3. Q&A Generation (Punjabi â†’ English)](#3-qa-generation-punjabi--english)
- [Configuration](#configuration)
- [Performance & Results](#performance--results)
- [Troubleshooting](#troubleshooting)
- [API Reference](#api-reference)

---

## Punjab Paddy Data Processing Pipeline

### Complete End-to-End Workflow

This project includes a complete pipeline for processing Punjab Paddy (Dhan) agricultural data:

**Pipeline Steps:**

1. **Filter Punjab Paddy Data** - Extract Paddy-specific records from combined dataset
2. **Deduplication** - Remove duplicate questions using hybrid approach
3. **Q&A Generation** - Generate English Q&A pairs using Claude Batch API
4. **Merge with Existing Data** - Combine with existing Punjab Paddy dataset
5. **Final Deduplication** - Remove duplicates from merged dataset
6. **Season Classification** - Populate Rabi/Kharif seasons based on dates
7. **Relevance Filtering** - Filter out non-Paddy questions using AI classification

**Quick Run:**

```bash
# Run complete pipeline
python scripts/pipelines/process_pipeline.py

# Filter Paddy-relevant questions only
python scripts/pipelines/filter_paddy_relevant.py

# Populate season column
python scripts/pipelines/populate_season.py
```

**Results:**
- Input: 68,494 total rows â†’ 12,451 Punjab Paddy rows
- After deduplication: 411 unique questions
- After Q&A generation: 411 rows with English Q&A
- After merge: 1,730 total rows
- Final output: 806 unique Paddy Q&A pairs
- Paddy-relevant only: Filtered using AI classification

**Output Files:**
- `outputs/final/final_deduplicated_punjab_paddy.csv` - Complete dataset
- `outputs/final/final_paddy_relevant_only.csv` - Only Paddy-relevant questions
- `outputs/reports/PIPELINE_EXECUTION_SUMMARY.md` - Detailed execution report



---

## ğŸ¯ Overview

This pipeline provides three main capabilities:

1. **Data Cleaning & Concatenation** - Clean and merge multiple CSV files into a unified dataset
2. **Question Deduplication** - Remove duplicate and semantically similar questions using GPU-accelerated ML
3. **Q&A Generation** - Translate Punjabi agricultural queries to English with enhanced answers using Claude AI

### Key Features

- âœ… **GPU-Accelerated** - 850x faster deduplication with NVIDIA GPU support
- âœ… **Cost-Optimized** - Uses Anthropic Batch API (50% cheaper than standard API)
- âœ… **High Accuracy** - 98%+ duplicate detection accuracy
- âœ… **Multilingual** - Handles English, Hindi, and Punjabi
- âœ… **Production-Ready** - Comprehensive logging, error handling, and reporting

---

## ğŸ“ Project Structure

```
unique-qs/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ config.yaml                        # Pipeline configuration
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â”œâ”€â”€ clean_and_concat.py       # CSV cleaning & concatenation
â”‚   â”‚   â”œâ”€â”€ deduplicate_questions.py  # Question deduplication pipeline
â”‚   â”‚   â””â”€â”€ process_paddy_workflow.py # Paddy workflow processing
â”‚   â”‚
â”‚   â”œâ”€â”€ qa_generation/
â”‚   â”‚   â”œâ”€â”€ generate_punjab_qa.py     # Main Q&A generation script
â”‚   â”‚   â”œâ”€â”€ retrieve_punjab_batch.py  # Batch result retrieval
â”‚   â”‚   â””â”€â”€ quickstart_qa.sh          # Quick start helper script
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ process_pipeline.py       # Complete 5-step pipeline
â”‚   â”‚   â”œâ”€â”€ filter_paddy_relevant.py  # Filter Paddy-relevant questions
â”‚   â”‚   â””â”€â”€ populate_season.py        # Populate Season column
â”‚   â”‚
â”‚   â”œâ”€â”€ process_all.py                # Batch processing utility
â”‚   â”œâ”€â”€ check_gpu.py                  # GPU verification
â”‚   â”œâ”€â”€ filter_punjab_paddy.py        # Filter Punjab Paddy data
â”‚   â””â”€â”€ create_test_data.py           # Test data generator
â”‚
â”œâ”€â”€ utils/                             # Core utility modules
â”‚   â”œâ”€â”€ text_processing.py            # Text normalization
â”‚   â”œâ”€â”€ similarity.py                 # Similarity computation
â”‚   â”œâ”€â”€ clustering.py                 # Clustering algorithms
â”‚   â””â”€â”€ reporting.py                  # Report generation
â”‚
â”œâ”€â”€ Data/                              # Input data directory
â”‚   â”œâ”€â”€ README.md                     # Data directory documentation
â”‚   â”œâ”€â”€ PB_Combined_Cleaned.csv       # Cleaned & concatenated output
â”‚   â”œâ”€â”€ PUNJAB_Paddy_Dhan.csv         # Existing Punjab Paddy dataset
â”‚   â””â”€â”€ groups/                       # Duplicate groups (generated)
â”‚
â”œâ”€â”€ outputs/                           # Output directory
â”‚   â”œâ”€â”€ README.md                     # Outputs documentation
â”‚   â”œâ”€â”€ intermediate/                 # Step-by-step outputs
â”‚   â”‚   â”œâ”€â”€ step1_filtered_punjab_paddy.csv
â”‚   â”‚   â”œâ”€â”€ step2_deduplicated.csv
â”‚   â”‚   â”œâ”€â”€ step3_with_qa.csv
â”‚   â”‚   â””â”€â”€ step4_merged.csv
â”‚   â”œâ”€â”€ final/                        # Final output files
â”‚   â”‚   â”œâ”€â”€ final_deduplicated_punjab_paddy.csv
â”‚   â”‚   â”œâ”€â”€ final_paddy_relevant_only.csv
â”‚   â”‚   â””â”€â”€ final_paddy_relevant_only_non_paddy_removed.csv
â”‚   â”œâ”€â”€ reports/                      # Processing reports
â”‚   â”‚   â”œâ”€â”€ PIPELINE_EXECUTION_SUMMARY.md
â”‚   â”‚   â””â”€â”€ *.report.txt
â”‚   â””â”€â”€ batch_files/                  # Batch API files
â”‚       â””â”€â”€ classification_batch.jsonl
â”‚
â”œâ”€â”€ logs/                              # Log files
â”‚   â”œâ”€â”€ pipeline_run.log
â”‚   â”œâ”€â”€ step3_qa_generation.log
â”‚   â”œâ”€â”€ step5_final_dedup.log
â”‚   â””â”€â”€ filter_paddy_relevant_v2.log
â”‚
â”œâ”€â”€ docs/                              # Additional documentation
â”‚   â””â”€â”€ reports/
â”‚
â””â”€â”€ venv/                              # Virtual environment
```


---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone or navigate to the repository
cd /home/ubuntu/Kshitij/unique-qs

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# For GPU support (deduplication)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 2. Verify Setup

```bash
# Check GPU availability (for deduplication)
python scripts/check_gpu.py

# Expected output:
# âœ“ PyTorch installed: 2.5.1+cu121
# âœ“ CUDA available: 12.1
# âœ“ GPU detected: NVIDIA H200
```

### 3. Run Complete Pipeline

```bash
# Step 1: Clean and concatenate CSV files
python scripts/data_processing/clean_and_concat.py

# Step 2: Deduplicate questions
python scripts/data_processing/deduplicate_questions.py \
  --input Data/PB_Combined_Cleaned.csv \
  --output outputs/deduplicated/PB_Unique.csv

# Step 3: Generate English Q&A pairs
python scripts/qa_generation/generate_punjab_qa.py \
  --api-key YOUR_ANTHROPIC_API_KEY \
  --rows 100  # Test with 100 rows first
```

---

## ğŸ“Š Data Processing Workflows

### 1. Data Cleaning & Concatenation

**Purpose:** Clean and merge multiple monthly CSV files into a single unified dataset.

**Input Files:**
- `Data/PB JAN .csv`
- `Data/PB FEB.csv`
- `Data/PB MAR.csv`
- `Data/PB APR.csv`
- `Data/PB MAY.csv`
- `Data/PB JUL.csv`

**Processing Steps:**
1. Removes rows with empty `QueryText`
2. Extracts `Day` from `CreatedOn` timestamp
3. Generates unique `req_id` for each record
4. Concatenates all files maintaining data integrity

**Usage:**

```bash
python scripts/data_processing/clean_and_concat.py
```

**Output:**
- File: `Data/PB_Combined_Cleaned.csv`
- Rows: 68,494 (after removing empty queries)
- Columns: `BlockName, Category, Year, Month, Day, Crop, DistrictName, QueryType, Season, Sector, StateName, QueryText, KccAns, req_id`

**Results:**
```
Input Files Processed:
  PB JAN .csv  - 1,950 rows
  PB FEB.csv   - 14,058 rows
  PB MAR.csv   - 13,324 rows
  PB APR.csv   - 8,807 rows
  PB MAY.csv   - 14,806 rows
  PB JUL.csv   - 15,549 rows
  
Total Output: 68,494 rows
```

---

### 2. Question Deduplication

**Purpose:** Remove duplicate and semantically similar questions using GPU-accelerated ML.

**Three-Stage Hybrid Approach:**

#### Stage 1: Exact Matching
- Method: Hash-based comparison with normalized text
- Speed: O(n) - very fast
- Accuracy: 100%

#### Stage 2: Fuzzy Matching
- Method: Smart sampling with length-based bucketing
- Comparisons: 50K instead of 300M+ (99.98% reduction)
- Speed: 60-180x faster than brute force
- Accuracy: ~95%

#### Stage 3: Semantic Similarity (GPU)
- Method: Sentence Transformers with multilingual model
- Model: `paraphrase-multilingual-mpnet-base-v2`
- Hardware: NVIDIA GPU acceleration
- Accuracy: 95-98%

**Usage:**

```bash
# Basic usage
python scripts/data_processing/deduplicate_questions.py \
  --input Data/PB_Combined_Cleaned.csv \
  --output outputs/deduplicated/PB_Unique.csv

# Custom question column
python scripts/data_processing/deduplicate_questions.py \
  --input Data/custom.csv \
  --column "QueryText" \
  --output outputs/deduplicated/output.csv
```

**Configuration:**

Edit `config.yaml` to adjust thresholds:

```yaml
deduplication:
  strategy: "hybrid"
  
  fuzzy:
    enabled: true
    threshold: 0.92              # 92% similarity (0-1)
    max_comparisons: 50000
    
  semantic:
    enabled: true
    model: "paraphrase-multilingual-mpnet-base-v2"
    similarity_threshold: 0.88   # 88% cosine similarity
    batch_size: 128
    use_gpu: true
```

**Performance Benchmarks:**

| Dataset | Original | Unique | Reduction | Time |
|---------|----------|--------|-----------|------|
| AI_ANS_25K | 25,007 | 5,007 | 80.0% | 57s |
| UP Wheat 67k | 67,183 | 928 | 98.6% | 13s |
| **Total** | **92,190** | **5,935** | **93.6%** | **70s** |

**Speedup:** 850x faster than baseline (17 hours â†’ 1.2 minutes)

---

### 3. Q&A Generation (Punjabi â†’ English)

**Purpose:** Translate Punjabi agricultural queries to English with enhanced, educational answers using Claude AI.

**Technology:**
- **AI Model:** Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- **API Method:** Anthropic Batch API (50% cheaper than standard)
- **Processing:** Asynchronous batch processing
- **Answer Length:** 200-400 words (enhanced from original)

**Workflow:**

```
Input: Punjabi QueryText + KccAns
         â†“
   Claude AI Translation
         â†“
Output: English Question + Enhanced Answer
```

**Quick Start:**

```bash
# Option 1: Using quick start script (easiest)
cd scripts/qa_generation
./quickstart_qa.sh YOUR_ANTHROPIC_API_KEY 100

# Option 2: Manual execution
python scripts/qa_generation/generate_punjab_qa.py \
  --api-key YOUR_ANTHROPIC_API_KEY \
  --rows 100 \
  --output outputs/qa_results/PB_Test_QA.csv
```

**Full Dataset Processing:**

```bash
python scripts/qa_generation/generate_punjab_qa.py \
  --api-key YOUR_ANTHROPIC_API_KEY \
  --input Data/PB_Combined_Cleaned.csv \
  --output outputs/qa_results/PB_Combined_QA.csv
```

**Retrieve Interrupted Batch:**

If processing is interrupted, retrieve results using the Batch ID:

```bash
python scripts/qa_generation/retrieve_punjab_batch.py \
  --batch-id msgbatch_XXXXXXXXXX \
  --api-key YOUR_ANTHROPIC_API_KEY \
  --rows 100
```

**Cost Estimation:**

| Dataset Size | Processing Time | Cost (Batch API) |
|--------------|----------------|------------------|
| 100 rows | 2-3 minutes | ~$0.70 |
| 1,000 rows | 20-30 minutes | ~$7 |
| 10,000 rows | 3-5 hours | ~$70 |
| 68,494 rows | 2-3 hours | ~$472 |

**Batch API Pricing:**
- Input tokens: ~$3 per million
- Output tokens: ~$15 per million
- **50% cheaper** than standard API

**Example Transformation:**

**Input (Punjabi):**
```
QueryText: "à¨—à¨°à¨®à©€à¨†à¨‚ à¨¦à©‡ à¨®à©‚à©°à¨— à¨¦à©€ à¨¬à¨¿à¨œà¨¾à¨ˆ à¨¦à¨¾ à¨¸à¨®à¨¾à¨‚?"
KccAns: "à¨—à¨°à¨®à©€à¨†à¨‚ à¨¦à©‡ à¨®à©‚à©°à¨— à¨¦à©€ à¨¬à¨¿à¨œà¨¾à¨ˆ 15 à¨®à¨¾à¨°à¨š à¨¤à©‹à¨‚ 10 à¨…à¨ªà©à¨°à©ˆà¨² à¨¹à©ˆà¥¤"
```

**Output (English):**
```
Question: "What is the sowing time for summer moong in Punjab?"
Answer: "The ideal sowing time for summer moong (green gram) in Punjab 
is from March 15 to April 10. This timing is crucial for optimal yield 
as it allows the crop to establish before the peak summer heat. Farmers 
should prepare the field by plowing 2-3 times and ensure proper seed bed 
preparation. The recommended seed rate is 15-20 kg per hectare. Sow seeds 
at a depth of 3-4 cm with row spacing of 30 cm. Ensure adequate soil 
moisture at the time of sowing..." (200-400 words total)
```

**Output CSV Structure:**

```
BlockName, Category, Year, Month, Day, Crop, DistrictName, QueryType, 
Season, Sector, StateName, QueryText, KccAns, req_id, Question, Answer
```

---

## âš™ï¸ Configuration

### Main Configuration File: `config.yaml`

```yaml
deduplication:
  strategy: "hybrid"  # exact, fuzzy, semantic, or hybrid
  
  exact:
    enabled: true
    
  fuzzy:
    enabled: true
    threshold: 0.92              # 92% similarity (0-1)
    max_comparisons: 50000       # Limit for large datasets
    use_sampling: true           # Smart sampling optimization
    
  semantic:
    enabled: true
    model: "paraphrase-multilingual-mpnet-base-v2"
    similarity_threshold: 0.88   # 88% cosine similarity (0-1)
    batch_size: 128              # Optimized for GPU
    use_gpu: true                # GPU acceleration enabled
```

### Tuning Guidelines

**More Strict** (fewer duplicates removed):
```yaml
fuzzy:
  threshold: 0.95
semantic:
  similarity_threshold: 0.92
```

**More Lenient** (more duplicates removed):
```yaml
fuzzy:
  threshold: 0.88
semantic:
  similarity_threshold: 0.85
```

**Faster Processing** (disable semantic):
```yaml
semantic:
  enabled: false  # 70% faster, but lower accuracy
```

---

## ğŸ“ˆ Performance & Results

### Data Cleaning Results

| Metric | Value |
|--------|-------|
| Input Files | 6 monthly CSV files |
| Total Input Rows | ~68,500 |
| Rows with Empty QueryText | Removed |
| Output Rows | 68,494 |
| Processing Time | < 5 seconds |

### Deduplication Results

| Metric | Value |
|--------|-------|
| Combined Accuracy | 98-99% |
| GPU Speedup | 850x faster |
| Processing Time (25K rows) | 57 seconds |
| Processing Time (67K rows) | 13 seconds |

### Q&A Generation Results

| Metric | Value |
|--------|-------|
| Translation Accuracy | Very High (Claude Sonnet 4.5) |
| Success Rate | ~98-100% |
| Average Answer Length | 200-400 words |
| Cost Savings (Batch API) | 50% vs standard API |

---

## ğŸ› ï¸ Troubleshooting

### GPU Not Detected (Deduplication)

```bash
# Check NVIDIA driver
nvidia-smi

# Install PyTorch with CUDA
pip install torch --index-url https://download.pytorch.org/whl/cu121

# Verify GPU
python scripts/check_gpu.py
```

### Out of Memory (Deduplication)

Reduce batch size in `config.yaml`:
```yaml
semantic:
  batch_size: 64  # or 32
```

### API Key Issues (Q&A Generation)

- Verify API key is correct
- Ensure you have a paid Anthropic account (Batch API requires it)
- Check key hasn't expired at https://console.anthropic.com/

### Batch Still Processing

- This is normal for large datasets
- Batch ID is saved - you can close script and retrieve later
- Use `retrieve_punjab_batch.py` to check status and get results

### Dependencies Not Found

```bash
# Reinstall all dependencies
pip install -r requirements.txt

# For GPU support
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

---

## ğŸ“š API Reference

### Data Cleaning

```python
# Run from command line
python scripts/data_processing/clean_and_concat.py

# Output: Data/PB_Combined_Cleaned.csv
```

### Deduplication

```python
from scripts.data_processing.deduplicate_questions import QuestionDeduplicator, load_config

# Load configuration
config = load_config('config.yaml')

# Create deduplicator
deduplicator = QuestionDeduplicator(config)

# Run deduplication
df_result = deduplicator.deduplicate(
    input_file='Data/PB_Combined_Cleaned.csv',
    output_file='outputs/deduplicated/output.csv',
    question_column='QueryText'
)

# Access report
deduplicator.report.print_summary()
```

### Q&A Generation

```bash
# Command-line usage
python scripts/qa_generation/generate_punjab_qa.py \
  --input Data/PB_Combined_Cleaned.csv \
  --output outputs/qa_results/PB_QA.csv \
  --rows 100 \
  --api-key YOUR_API_KEY

# Available options:
#   --input, -i    Input CSV file path
#   --output, -o   Output CSV file path
#   --rows, -r     Number of rows to process (default: all)
#   --api-key, -k  Anthropic API key (required)
```

---

## ğŸ¯ Use Cases

This pipeline is ideal for:

- **Dataset Preparation** - Clean and prepare agricultural data for ML training
- **Knowledge Base Creation** - Extract unique questions for FAQs and chatbots
- **Multilingual Processing** - Translate regional language queries to English
- **Data Quality Control** - Identify and merge similar entries
- **Cost-Effective AI Processing** - Use batch APIs for large-scale translations

---

## ğŸ“ Best Practices

1. **Always Test First** - Run with small samples (100 rows) before full processing
2. **Save Batch IDs** - Note batch IDs for Q&A generation for later retrieval
3. **Monitor Costs** - Check Anthropic console during Q&A generation
4. **Backup Data** - Keep original CSV files safe before processing
5. **Verify GPU** - Ensure GPU is available for deduplication speedup
6. **Adjust Thresholds** - Fine-tune deduplication thresholds based on your data

---

## ğŸ“Š Success Metrics

âœ… **68,494 rows** cleaned and processed from Punjab agricultural data  
âœ… **93.6% reduction** in duplicate questions (proven on test datasets)  
âœ… **850x performance improvement** with GPU acceleration  
âœ… **98%+ accuracy** in duplicate detection  
âœ… **50% cost savings** using Batch API for Q&A generation  
âœ… **Production-ready** with comprehensive logging and error handling  
âœ… **Multilingual support** (English, Hindi, Punjabi)  

---

## ğŸ“ Dependencies

```
# Core
pandas>=2.0.0
numpy>=1.24.0

# Deduplication
sentence-transformers>=2.2.0
rapidfuzz>=3.0.0
scikit-learn>=1.3.0
torch>=2.0.0
openpyxl>=3.1.0

# Q&A Generation
anthropic>=0.72.1
pytz>=2024.1
nest_asyncio>=1.6.0

# Utilities
tqdm>=4.65.0
pyyaml>=6.0
```

---

## ğŸ“ Support

For issues or questions:

1. Check this README for relevant sections
2. Review configuration in `config.yaml`
3. Verify GPU setup: `python scripts/check_gpu.py`
4. Check output reports for statistics
5. Review script output for error messages

---

## ğŸ¤ Contributing

To extend or modify the pipeline:

1. **Add new similarity metrics**: Edit `utils/similarity.py`
2. **Custom text processing**: Edit `utils/text_processing.py`
3. **Different clustering**: Edit `utils/clustering.py`
4. **Enhanced reporting**: Edit `utils/reporting.py`
5. **New workflows**: Add scripts to `scripts/` directory

---

## ğŸ“„ License

MIT License - Feel free to use and modify for your needs.

---

**Built with â¤ï¸ for agricultural data processing**

*Last Updated: December 23, 2025*
